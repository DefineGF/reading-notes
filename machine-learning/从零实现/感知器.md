##### 激活函数

$$
f(z) = 1 \space if \space z > 0 \space else \space 0
$$



##### 输出计算

$$
y = f(w*x +b)
$$





##### 参数更新

$$
w'_{i} = w_{i} + η*(t - y)*x_{i} \\ 
\\
b'_{i} = b_{i} + η*(t - y)
$$





#### &#x20;实现

```python
class Perceptron(object):
    def __init__(self, input_num, activator):
        self.activator = activator 					   # 设置激活函数     
        self.weights = [0.0 for _ in range(input_num)] # 权重向量初始化为0
        self.bias = 0.0 							   # 偏置项初始化为0

    def __str__(self): # 打印偏重
        return 'weights\t:%s\nbias\t:%f\n' % (self.weights, self.bias)

    def predict(self, input_vec): # 
		# y = f(w * x + b)
        return self.activator(
            reduce(lambda a, b: a + b,
                   map(lambda x, w: x * w, input_vec, self.weights), 0.0)
            + self.bias)

    def train(self, input_vecs, labels, iteration, rate):
        for i in range(iteration):
            self._one_iteration(input_vecs, labels, rate)

    def _one_iteration(self, input_vecs, labels, rate):
        for (input_vec, label) in zip(input_vecs, labels):
            output = self.predict(input_vec)
            self._update_weights(input_vec, output, label, rate) # 更新权重

    def _update_weights(self, input_vec, output, label, rate):
        delta = label - output
        self.weights = list(map(lambda x, w: w + rate * delta * x, input_vec, self.weights)) # 注意 py2 和 py3 的区别
        self.bias += rate * delta
```

### 测试

#### 实现 and 机制

and机制:&#x20;

*   1 & 1 = 1;
*   1 & 0 = 0;
*   0 & 1 = 0;
*   0 & 0 = 0;

```python
# 获取数据
def get_and_training_dataset():
    input_vecs = [[1, 1], [0, 0], [1, 0], [0, 1]]
    labels = [1, 0, 0, 0]
    return input_vecs, labels

# 训练感知器
def f(x):					
    return 1 if x > 0 else 0			# 激活函数

def train_perceptron(input_vecs, labels):
    p = Perceptron(2, f)
    p.train(input_vecs, labels, 10, 0.1) # 训练，迭代10轮, 学习速率为0.1
    return p

# 输出测试
def and_perceptron_test():
    inputs, labels = get_and_training_dataset()
    and_perception = train_perceptron(inputs, labels)
    print(and_perception)
    # 测试
    print('1 and 1 = %d' % and_perception.predict([1, 1]))
    print('0 and 0 = %d' % and_perception.predict([0, 0]))
    print('1 and 0 = %d' % and_perception.predict([1, 0]))
    print('0 and 1 = %d' % and_perception.predict([0, 1]))
```

#### 实现一次函数 y = x/2 + 1

```python
# 生成数据: 随机生成 x, y, 并根据函数计算真值, 在直线之上标签为1;
def get_func_training_dataset():
    len = 128
    x = [random.random() * 10 for _ in range(len)]
    y = [random.random() * 5 + 1 for _ in range(len)]
    labels = list(map(lambda x, y: 1 if y > (x / 2 + 1) else 0, x, y))
    inputs = list(zip(x, y))
    print("inputs: " + str(inputs))
    print("labels: " + str(labels))
    
    # 为直观可以绘制出来
    # # 绘图
    # fig = plt.figure()
    # ax1 = fig.add_subplot(1, 1, 1)
    # # 散点图
    # ax1.scatter(x, y)
    # # 直线
    # _x = [x for x in range(12)]
    # _y = [x/2 + 1 for x in _x]
    # ax1.plot(np.array(_x), np.array(_y))
    # plt.show()
    return inputs, labels

```

测试:

```python
def func_perceptron_test():
    inputs, labels = get_func_training_dataset()
    func_perception = train_perceptron(inputs, labels)

    # 测试是否准确
    len = 64
    x = [random.random() * 10 + 5 for _ in range(len)]
    y = [random.random() * 6 + 3 for _ in range(len)]
    labels = list(map(lambda x, y: 1 if y > (x / 2 + 1) else 0, x, y))

    error_count = 0
    for (_x, _y, _ans) in zip(x, y, labels):
        pred = func_perception.predict([_x, _y])
        print("%f _ %f, 预测: %d, 真值: %d" % (_x, _y, pred, _ans))
        if pred != _ans:
            error_count += 1
    print("错误率: %f" % (error_count / len))
```

当训练轮数设置为32 时, 错误率为0, 输出参数为:

> weights: \[-3.388496824735921, 6.75760808450439]
> bias:    -6.700000

也就是说:

> f(w1 \* x + w2 \* y + b)

直观形式:
$$
y = -\frac{w1}{w2}x - \frac{b}{w2} = 1/2 * x + 1
$$




