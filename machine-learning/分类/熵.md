#### 前言

##### 信息量

$$
I(x) = - ln(P(x))
$$

##### 信息熵

$$
H(x) = \sum_{i=1}^nP(x_i)ln(P(x_i))
$$

##### 相对熵（KL散度）：衡量两个概率分布之间的差异

$$
D_{kl} (p||q) = \sum_{i=1}^nP(x_i)ln(\frac{P(x_i)}{Q(x_i)})
$$

例如：

P(x) 样本真实分布：[1, 0, 0]

Q(x) 模型预测分布：[0.7, 0.2, 0.1]

那么KL散度D<sub>kl</sub> (p||q) = 1 * ln(1 / 0.7) = 0.36; 其中KL越小，表示P(x) 和 Q(x) 越接近。



##### 交叉熵

$$
H(p, q) = - \sum_{i=1}^nP(x_i)ln(Q(x_i))
$$

又：
$$
\begin{aligned}
D_{kl} (p||q) &= \sum_{i=1}^nP(x_i)ln(\frac{P(x_i)}{Q(x_i)}) \\
&= \sum_{i=1}^nP(x_i)ln(P(x_i)) - \sum_{i=1}^nP(x_i)ln(Q(x_i)) \\
&= -H(P(x_i)) + H(p, q)
\end{aligned}
$$

即：KL散度 = 交叉熵 - 信息熵；

**交叉熵作为loss函数：**

训练神经网络时，输入数据和标签已确定，其真实概率分布P(x<sub>i</sub>) 也确定了。因此信息熵H(P(x<sub>i</sub>)为常量。于是可以通过交叉熵来确定真实概率与预测概率之间的差异。

例：

| 类别  | 猫   | 狗   | 马   |
| ----- | ---- | ---- | ---- |
| label | 0    | 1    | 0    |
| pred  | 0.2  | 0.7  | 0.1  |

loss = -（0 * ln(0.2) + 1 * ln(0.7) +0 * ln(0.1)) = 0.36



 