### KNN（K-Nearest Neighbor)

原理：如果一个样本在一个特征空间的k个最相似的（最临近）的样本中的大多数属于某个类别，则该样本也属于该类别。



#### 决策过程

1. 将新数据的每个特征与样本集中数据对应的特征比较；
2. 提取样本集中特征最相似（最临近）的K（一般不超过20）个数据的分类标签；
3. 选择K个最相似数据中出现次数最多的分类标签，最为新数据的分类标签（多数投票表决）；



#### K值影响

##### 较小的K值

- 较少的训练实例，非常相似的实例才起作用，学习的**近似误差**会减小；
- 预测实例与少量实例有关，对近邻数据非常敏感，学习的**估计误差**会增大；
- 噪声敏感；
- 容易过拟合

##### 较大的K值

- 较多的训练实例进行预测，学习的估计误差会减小；
- 与输入数据较远的实例也会起作用，学习的近似误差会增大；
- K值增大意味着模型变简单，容易欠拟合；



#### 算法流程

对未知类别的数据集中的每个数据点依次执行以下操作：

1. 计算已知类别数据集中的点与当前点之间的距离；
2. 选取与当前点最相似的K个点 并 确定其类别；
3. 返回前K个点出现次数或频率最高的类别作为预测分类；



#### KNN算法扩展

##### 数值归一化

| 样本 | 特征1 | 特征2 | 特征3 | 分类 |
| ---- | ----- | ----- | ----- | ---- |
| 0    | 0     | 20000 | 1.1   | 2    |
| 1    | 67    | 32000 | 0.1   | 1    |

样本之间的距离：
$$
\sqrt{(0-67)^2+(20000 - 32000)^2+(1.1-0.1)^2}
$$
因此会出现特征中某一特征影响远远大于其他两个特征。解决方法：
$$
new\_value = \frac{value - min}{max - min}
$$

##### 根据远近距离做权值修改

当样本不均衡时，某些类的样本量很大，而某些类的样本量很小。

当输入未知样本时，K个邻居中大数量类的样本占多数，虽然这类样本并不接近未知样本；