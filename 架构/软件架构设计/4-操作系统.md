#### IO

##### 概念

- 应用程序内存：通常通过 malloc/free、new/delete 等分配出来的；

- 用户缓冲区：C语⾔的FILE结构体⾥⾯的buffer；

  ```c
  // stdio.h
  typedef struct {
      int file_descriptor;     // 文件描述符
      char* buffer;            // 文件缓冲区指针
      int buffer_size;         // 缓冲区大小
      int position;            // 当前文件位置指针
      // 其他用于表示文件状态和控制文件操作的成员
  } FILE;
  ```

- 内核缓冲区：linux中的page cache：Linux系统会把磁盘上的数据以Page为单位缓存在操作系统的内存⾥；



##### 缓冲io

缓冲I/O是C语⾔提供的库函数，均以f打头：fopen、fclose、fseek、fread、fscanf、fflush等等；

- 读：磁盘→内核缓冲区→⽤户缓冲区→应⽤程序内存；
- 写：应⽤程序内存→⽤户缓冲区→内核缓冲区→磁盘；

其中：

- fflush：把数据从⽤户缓冲区刷到内核缓冲区⽽已；
- fsync：数据从内核缓冲区刷到磁盘⾥；

**如果在写数据之后不调⽤fsync，此时系统断电重启，最新的部分数据会丢失，因为数据只是在内核缓冲区⾥⾯，操作系统还没来得及刷到磁盘。**



##### 直接io

直接I/O是Linux的系统API，比如 open、close等；

- 读：磁盘→内核缓冲区→应⽤程序内存；



> 当需要将文件中的数据发送到网络时：磁盘→内核缓冲区→应⽤程序内存→Socket缓冲区→⽹络



##### 内存映射

应⽤程序的**逻辑内存地址**映射到 Linux 操作系统的**内核缓冲区**，应⽤程序虽然读写的是⾃⼰的内存，但这个内存只是⼀个“逻辑地址”，实际读写的是内核缓冲区。

- 读：磁盘→内核缓冲区；
- 写：内核缓冲区→磁盘；

由于应用程序访问逻辑内存地址会映射到内核缓冲区，因此读写操作相当于直接对内核缓冲区进行读写，减少数据拷贝。

> 当需要将文件中的数据发送到网络时：
>
> 直接在内核空间中从内核缓冲区拷贝到Socket缓冲区。



##### 零拷贝

在内核缓冲区和Socket缓冲区之间进行**地址映射**！

底层的⽹卡驱动程序要读取数据并发送到⽹络的时候，看似读的是Socket缓冲区的数据，但实际上直接读的是内核缓冲区中的数据；



#### 网络io模型

##### 阻塞与非阻塞

- 阻塞：如果读写没有就绪或者读写没有完成，则该函数⼀直等待；
- 非阻塞：函数⽴即返回（想要获取相关结果，则执行轮询）；



##### 同步与异步

- 同步：读写由应⽤程序完成；
- 异步：读写由操作系统完成，完成之后，回调或者事件通知应⽤程序；



##### io多路复用

- select
- poll
- epoll

>  详情参见 linux 部分



##### 异步io

读写都是由操作系统完成的，然后通过回调函数或者某种其他通信机制通知应⽤程序。

> 真正的异步io：Windows系统的IOCP；



#### 网络框架

##### reactor：主动模式

所谓主动，是指应⽤程序不断地轮询，询问操作系统或者⽹络框架、I/O是否就绪。

- Linux系统下的select、poll、epoll就属于主动模式，需要应⽤程序中有⼀个循环⼀直轮询；

- Java中的NIO也属于这种模式。在这种模式下，实际的I/O操作还是应⽤程序执⾏的；



##### proactor：被动模式

应⽤程序把read和write函数操作全部交给操作系统或者⽹络框架，实际的 I/O 操作由操作系统或⽹络框架完成，之后再回调应⽤程序；



#### 进程 & 线程 & 协程

##### 多线程

服务器端的程序往往是I/O密集型的应⽤（单个死循环的CPU计算就能占满CPU核）：

- 提⾼CPU利⽤率：当⼀个线程发⽣I/O时，会把该线程从CPU上调度下来，并把其他的线程调度上去，继续计算；
- 提高IO吞吐：以Redis和MySQL为例，通过连接池和多线程，实现每个线程使⽤⼀个连接；



##### 多进程

不要通过共享内存来实现通信，⽽应通过通信实现共享内存。

- 进程间由于资源隔离，可以避免多线程之间的加锁等操作；

- ⼀是减少了多线程在不同的CPU核间切换的开销：单个进程可以绑定到指定的cpu；

  > ```cpp
  > // 创建一个CPU集合，表示要绑定的CPU
  > cpu_set_t cpuset;
  > CPU_ZERO(&cpuset);
  > CPU_SET(0, &cpuset);  // 绑定到CPU 0
  > 
  > // 获取当前进程的ID
  > pid_t pid = getpid();
  > 
  > // 设置进程与CPU的绑定
  > int result = sched_setaffinity(pid, sizeof(cpuset), &cpuset);
  > if (result == -1) {
  >     std::cerr << "Failed to set CPU affinity" << std::endl;
  >     return 1;
  > }
  > ```

- 多进程相互独⽴，意味着其中⼀个崩溃后，其他进程可以继续运⾏;

  > 线程崩溃可能会引发一些间接的影响，如资源的竞争或死锁。如果崩溃的线程持有某个共享资源的锁，并且其他线程试图访问该资源，那么这些线程可能会因为无法获取锁而被阻塞。这种情况下，其他线程可能会受到崩溃线程的间接影响而出现阻塞或死锁的情况。
  >
  > 此外，如果崩溃的线程是主线程（即程序的起始线程），那么整个进程可能会终止。这是因为主线程的崩溃会导致进程的异常终止，从而影响到其他所有线程。

##### 多协程

虽然线程切换的开销⽐进程切换的开销⼩很多，但还是不够。多协程：

- 更好地利⽤CPU：线程的调度是由操作系统完成的，应⽤程序⼲预不了，协程可以由应⽤程序⾃⼰调度；
- 更好地利⽤内存：协程的堆栈⼤⼩不是固定的，⽤多少申请多少，内存利⽤率更⾼；



##### java线程

Java 程序在运行时，会由 Java 虚拟机（JVM）将 Java 线程映射到底层操作系统的线程上进行执行。区别：

- 创建和销毁的开销：相对于操作系统线程，Java 线程的创建和销毁开销较大；

  > 1. 内存分配：创建一个 Java 线程需要为线程对象分配内存空间。线程对象包含了线程的状态、调用栈、局部变量等信息。相比之下，操作系统线程的内存分配通常是由操作系统负责管理，且开销较低。
  > 2. 初始化和启动：在创建 Java 线程后，需要进行一系列的初始化操作，如设置线程的优先级、绑定到特定的线程组、初始化线程的调用栈等。这些初始化操作需要占用一定的时间和资源。而操作系统线程的初始化过程相对较简单。
  > 3. Java 虚拟机参与：Java 线程的创建和销毁过程需要 Java 虚拟机（JVM）的参与。JVM 需要为每个线程分配资源并进行管理，包括分配堆栈内存、维护线程状态信息、进行线程调度等。这些额外的操作和管理会增加创建和销毁线程的开销。
  > 4. 上下文切换：Java 线程的上下文切换需要保存和恢复线程的执行状态，包括寄存器内容、堆栈指针等。相比之下，操作系统线程的上下文切换由操作系统负责，其开销往往较低。

- 调度方式：Java 线程的调度是由 Java 虚拟机的线程调度器（Thread Scheduler）负责的;



jvm如何将java线程映射到操作系统的线程上的?

> 1. 线程创建：当 Java 程序创建一个新的线程时，JVM 会为该线程分配内存，并在内存中创建线程对象（Thread Object）。这个线程对象包含了线程的状态、调用栈、局部变量等信息。
> 2. 线程对象和操作系统线程的关联：JVM 会将线程对象和操作系统线程进行关联。这个关联的具体实现可能因 JVM 实现而有所不同，**但通常会使用一个数据结构（如线程控制块）来存储线程对象与操作系统线程之间的映射关系**。
> 3. 调度和执行：当线程处于可运行状态时，JVM 的线程调度器选择一个线程来执行。选择的线程会被映射到一个操作系统线程上，即分配一个操作系统线程标识（Thread ID）和线程控制块。该操作系统线程获取操作系统分配的 CPU 时间片，并开始执行线程的代码。
> 4. 上下文切换：当线程执行时间片用完或发生阻塞时，线程调度器会决定切换到另一个可运行的线程。这个切换过程涉及到线程的上下文切换，即保存当前线程的执行上下文（如寄存器状态、堆栈指针等）并加载下一个线程的执行上下文



可以将多个java线程映射到同一操作系统线程上吗?

> 在标准情况下，不会将多个 Java 线程映射到同一操作系统线程上。每个 Java 线程都会被映射到一个独立的操作系统线程上，由操作系统负责线程的调度和执行。
>
> **特殊情况下**，多个 Java 线程会在同一个操作系统线程上轮流执行，以减少线程的创建和上下文切换开销。



#### 无锁

##### 内存屏障

在多核CPU体系下，每个CPU有⾃⼰的缓存！改过的这个值可能还在CPU的缓存⾥，没有刷新到内存⾥。内存屏障就是要强制把这个值刷新到内存⾥⾯。

具体而言，内存屏障可以分为读屏障和写屏障：

- 读屏障（如 smp_rmb）用于确保在屏障之前的读操作不会发生在屏障之后的读操作之前，以防止读取过期的数据；
- 写屏障（如 smp_wmb）用于确保在屏障之前的写操作不会发生在屏障之后的写操作之前，以防止写入未完成的数据被其他 CPU 看到；



> linux中的smp_wmb 是一种用于内存屏障的宏定义，对于 x86 架构的处理器，smp_wmb 宏定义通常会转换为一个编译器内置的指令 `__asm__ __volatile__("mfence" : : : "memory")`，该指令是 x86 架构中的内存屏障指令。
>
> `mfence` 操作会确保在该**指令之前的所有内存访问都完成**，并且在**该指令之后的所有内存访问都必须等待其完成**。阻止写屏障之后的内存操作进行指令重排和乱序执行，并确保之前的写操作对其他 CPU 可见。这样可以保证修改过的值被刷新到主内存中，以便其他 CPU 可以看到最新的值



以linux中RingBuffer为例：

> 满足多线程**读**，单线程**写**的线程安全操作。
>
> 有两个指针 fififo-＞in 和 fififo-＞out，分别对应队列的头部和尾部，写的线程操作fififo-＞in，读的线程操作fififo-＞out；
>
> 要保证先操作数据，先执⾏memcpy操作，后修改fififo-＞in或者fififo-＞out的值。为此，在memcpy和修改fififo-＞in/fififo-＞out之间插⼊了内存屏障。





