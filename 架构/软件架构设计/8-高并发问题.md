## 高并发

### 高并发读

场景：搜索引擎、商品搜索、商品展示；

#### 搜索引擎

- 数量级：浏览人数远高于编辑人数；
- 响应时间：读的一端通常要求在毫秒级；搜索引擎并不保证发布的文章一定被检索到；
- 频率

#### 高并发读策略

##### 1. 加缓存

- 本地缓存或者Memcached/Redis集中式缓存；

- MySQL的Master/Slave：

  > 对于有的场景，需要用到多张表的关联查询，比如各种后端的admin系统要操作复杂的业务数据，如果直接查业务系统的数据库，会影响C端用户的高并发访问;
  >
  > 1. 对于这种查询，往往会为MySQL加一个或多个 Slave，来分担主库的读压力，是一个简单而又很有效的办法;
  > 2. 当然，也可以把多张表的关联结果缓存成＜k，v＞，但这会存在一个问题：在多张表中，任何一张表的内容发生了更新，缓存都需要更新;

- CDN 动静分离

  > - 静态内容：对于不同的用户来说，数据基本是一样的，比如图片、HTML、JS、CSS 文件；再比如各种直播系统，内容生成端产生的视频内容，对于消费端来说，看到的都是一样的内容；
  > - 动态内容：需要根据用户的信息或其他信息（比如当前时间）实时地生成并返回给用户；
  >
  > 一个静态文件缓存到了全网的各个节点，当第一个用户访问的时候，离用户就近的节点还没有缓存数据，CDN就去源系统抓取文件缓存到该节点；等第二个用户访问的时候，只需要从这个节点访问即可，而不再需要去源系统取。

##### 2. 并发读：串行改并行

- 异步RPC；
- 冗余请求：客户端同时向多台服务器发送请求，哪个返回得快就用哪个，其他的丢弃，但这会让整个系统的调用量翻倍



##### 3. 重写轻读

场景1：微博feeds流

微博首页或微信朋友圈都存在类似的查询场景：用户关注了n个人（或者有n个好友），每个人都在不断地发微博，然后系统需要把这n个人的微博按时间排序成一个列表，也就是Feeds流并展示给用户。同时，用户也需要查看自己发布的微博列表。

SOLUTION1：

关注信息表：FOLLOWING

- id：自增id；
- user_id 关注者；
- following 被关注的人；

微博发布表：MSG

- id：自增id；
- user_id：发布者；
- msg_id：信息 id；

假如要查询 user_id = 1 的用户可观察到的微博列表（分页显示）：

```sql
select msg_id from MSG where user_id = 1; # 只查询自己的

# 查询关注者的
select following from FOLLOWING where user_id = 1;
select msg_id from MSG where user_id in (following) limit offset;
```

同时需要对两个表进行查询，同时将结果进行排序处理。



SOLUTION2：重写轻读

每个用户都有一个发件箱和收件箱。假设某个用户有1000个粉丝，发布1条微博后：

- 只写入自己的发件箱就返回成功；
- 后台异步地把这条微博推送到1000个粉丝的收件箱，也就是“写扩散”；

这样，每个用户读取Feeds流的时候不需要再实时地聚合了，直接读取各自的收件箱就可以。

> 当然需要持久化数据，即将用户发布的微博保存到 mysql 数据库中。
>
> 分片策略：
>
> - 根据 user_id 分片；
>
>   > 不能完全满足需求：假如id在 [1, 1000] 归 user_id = 1 的用户，那么当该用户不停发送微博超过 1001 时，并不能保证微博id = 1001 的唯一性；
>
> - 根据 时间分片；
>
>   > 会造成冷热不均：绝大部分读和写的请求都发生在当前月份里，历史月份的读请求很少，写请求则没有；
>
> - 混合分片策略：
>
>   > 结合用户ID和时间进行分片，将消息按照用户ID进行划分，并在每个用户的分片中按照时间进行进一步的分片。
>   >
>   > 这样可以在一定程度上解决用户数据和消息数据的均衡性问题。



附加要求1：如何快速地查看某个user_id 从某个offset开始的微博呢？

> 另外要有一张表，记录＜user_id，月份，count＞。也就是每个user_id在每个月份发表的微博总数。基于这个索引表才能快速地定位到offset=5000的微博发生在哪个月份，也就是哪个数据库的分片。



附加要求2：多表关联查询

- 宽表

  > 把要关联的表的数据算好后保存在宽表里；
  >
  > 依据实际情况，可以定时算，也可能任何一张原始表发生变化之后就触发一次宽表数据的计算；

- 搜索引擎：

  > 把多张表的Join结果做成一个个的文档，放在搜索引擎里面，也可以灵活地实现排序和分页查询功能



### 高并发写

场景：广告计费系统

#### 1. 数据分片

##### 数据库的分库分表

- 分表后，还是在一个数据库、一台机器上，但可以更充分地利用CPU、内存等资源；
- 分库后，可以利用多台机器的资源。

##### ConcurrentHashMap

内部分成了若干槽（个数是2的整数次方，默认是16个槽），也就是若干子HashMap。

这些槽可以并发地读写，槽与槽之间是独立的，不会发生数据互斥。



##### Kafka的partition

在Kafka中，一个topic表示一个逻辑上的消息队列；

具体到物理上，一个topic被分成了多个partition，每个partition对应磁盘中的一个日志文件。partition之间也是相互独立的，可以并发地读写，也就提高了一个topic的并发量。



#### 2. 任务分片

cpu的指令流水、MapReduce、Tomcat的 网络模型（1 + N + M）。



#### 3. 异步化

短信验证码注册或者登录、电商的订单系统、广告计费系统、LSM树、kafka的pipeline



#### 4. 批量

###### kafka

Kafka 的客户端在内存中为每个 Partition 准备了一个队列，称为RecordAccumulator。Producer 线程一条条地发送消息，这些消息都进入内存队列。然后通过Sender线程从这些队列中批量地提取消息发送给Kafka集群。



##### mysql

多个事务合并为一个事务。



#### 5. 串行化 + 多进程单线程 + 异步 io

有了异步I/O后，可以把请求串行化处理。第一，没有了锁的竞争；第二，没有了 I/O 的阻塞，这样单线程也非常高效；

多核情况下，可以使用多进程 + 单线程（将进程绑定到指定的核上）。



### 高并发读写

场景：电商库存系统和秒杀系统、支付系统和微信红包、及时通讯、微博和朋友圈；

#### 支付系统和微信红包

方面用户要实时地查看自己的账号余额（这个值需要实时并且很准确）；

另一方面用户A向用户B转账的时候，A 账户的扣钱、B 账户的加钱也要尽可能地快。



## 高可用

### 隔离

#### 数据隔离

- 在数据库的存储中，把这些数据所在的物理库彻底分开；
- 业务的拆库和数据的隔离，其实是从不同角度说同一个事情

#### 机器隔离

对于那些最核心的几个调用者，可以为其专门准备一组机器，这样其他的调用者不会影响该调用者的服务；

成熟的RPC框架往往有隔离功能，根据调用方的标识（ID），把来自某个调用方的请求都发送到一组固定的机器中，无须业务人员写代码，用一个简单的配置即可搞定。



#### 线程池隔离

Tomcat背后调用了很多的RPC服务，在这500个线程里面同步调用。现在假设某个服务的延迟突然变得很大，而这个服务的调用量又很大，很可能会导致500个线程都卡在RPC服务上，整个服务器也就卡死了。

可以使用线程池隔离，为每个RPC调用单独准备一个线程池（一般2～10个线程），而不是在这500个线程里同步调用。当线程池中没有空闲线程，并且线程池内部的队列也已经满了的情况下，线程池会直接抛出异常，拒绝新的请求，从而确保调用线程不会被阻塞。



### 限流

#### 技术层面的限流

- 一种是限制并发数，也就是根据系统的最大资源量进行限制，比如数据库连接池、线程池、Nginx的limit_conn模块；
- 另一种是限制速率（QPS），比如Guava的RateLimiter、Nginx的limit_req模块。过压力测试可以知道服务的QPS是2000，就可以限流为2000QPS。当调用方的并发量超过了这个数字，会直接拒绝提供服务。



#### 业务层面的限流

比如在秒杀系统中，一个商品的库存只有100件，现在有2万人抢购，没有必要放2万个人进来，只需要放前500个人进来，后面的人直接返回已售完即可。

> 针对这种业务场景，可以做一个限流系统，或者叫售卖的资格系统（票据系统），票据系统里面存放了500张票据，每来一个人，领一张票据。领到票据的人再进入后面的业务系统进行抢购；对于领不到票据的人，则返回已售完

#### 限流算法

- 漏桶
- 令牌桶





